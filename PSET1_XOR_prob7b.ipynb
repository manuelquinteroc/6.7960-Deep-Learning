{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelquinteroc/6.7960-Deep-Learning/blob/main/PSET1_XOR_prob7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAKE A COPY OF THIS COLAB**"
      ],
      "metadata": {
        "id": "Er8m1NSJQkjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) **XOR gate; 2pt** Construct a ReLU network with at most $3$ layers, each with width at most $4$ and $2$-dimensional inputs in $â„^2$  such that: \\begin{align}\n",
        "           f(x; W_1, W_2, b_1, b_2) > 0 \\iff\n",
        "           & (x_1 < 0 \\mathrel{\\mathrm{AND}} x_2 > 0) \\mathrel{\\mathrm{OR}} {} \\notag\\\\\n",
        "           & (x_1 > 0 \\mathrel{\\mathrm{AND}} x_2 < 0)\n",
        "       \\end{align}\n",
        "       \n",
        "       Write out the algebraic formula of *f* with explicit weight matrices and bias vectors.\n",
        "\n",
        "The code below is meant to allow you to test your answer and check if it is correct.\n",
        "**Don't forget to provide a justification/explaination of your derivation in your writeup!**"
      ],
      "metadata": {
        "id": "us1sQ0ZhRPGU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uBX9MHcPTQq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "def answer():\n",
        "    # Define ReLU function\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    # Define the function f(x)\n",
        "    def f(x1, x2):\n",
        "        x = np.array([x1, x2])\n",
        "\n",
        "        # First ReLU layer\n",
        "        W1 = # PUT ANSWER TO 7b HERE\n",
        "\n",
        "        h1 = # PUT ANSWER TO 7b HERE\n",
        "\n",
        "        # Second ReLU layer\n",
        "        W2 = # PUT ANSWER TO 7b HERE\n",
        "\n",
        "        h2 = # PUT ANSWER TO 7b HERE\n",
        "\n",
        "        # Final linear layer (if needed)\n",
        "        W3 = # PUT ANSWER TO 7b HERE\n",
        "\n",
        "\n",
        "        output = # PUT ANSWER TO 7b HERE\n",
        "        return output\n",
        "\n",
        "    # Create a grid of points for testing answer:\n",
        "    x1_vals = np.linspace(-2, 2, 100)\n",
        "    x2_vals = np.linspace(-2, 2, 100)\n",
        "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
        "    Z = np.zeros_like(X1)\n",
        "    title = \"Your XOR\"\n",
        "\n",
        "    return X1, X2, Z,title\n",
        "\n",
        "X1, X2, Z,title = answer()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the surface\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X1, X2, Z, cmap='viridis')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_zlabel('f(x1, x2)')\n",
        "ax.set_title(title)\n",
        "plt.show()\n"
      ]
    }
  ]
}